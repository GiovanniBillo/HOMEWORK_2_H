---
title: "GroupH_HM2"
author: "Simonutti, Younes Pour Langaroudi, Billo, Tavano, Vicig"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 3
date: "2024-12-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## FSDS - Chapter 4

### Ex 4.2

For a sequence of observations of a binary random variable, you observe the geometric random variable (Section 2.2.2) outcome of the first success on observation number $y$ = 3. Find and plot the likelihood function.

The probability mass function (PMF) of the **Geometric Distribution** is given by:

\[
P(Y = y) = (1 - \pi)^{y - 1} \pi, \quad y_{i} = 1, 2, 3, \dots
\]
and with $Y = 3$
\[
f(3) = (1 - \pi )^{3 - 1}\pi = (1 - \pi )^{2}\pi
\]
\[
l(\theta) = \prod_{i=1}^n f(y_i; \pi) = (1 - \pi )^{\sum_{i=1}^n (y_i - 1)}\pi
\]
In our case $\theta = \pi$
\[
l(\pi) = \pi(1 - \pi)^3
\]

```{r}
y = 3

my_llik <- function(y, pi){
  exponent = sum((1:y) - 1)
  return(pi*(1 - pi)^exponent)
}

pi_vec = seq(from = 0.1, to = 1, by = 0.01)
llik_vector = sapply(pi_vec, function(pi) my_llik(3, pi))

max_pi = print(pi_vec[which.max(llik_vector)])

plot(pi_vec, llik_vector, type = "l", col = "blue", lwd = 2, 
     xlab = expression(pi), ylab = "Likelihood", 
     main = "ML of a Geometric RV with Y = 3")


```
The graph shows that with Y = 3, meaning that it was necessary to have 3 trials before a success, the most likely parameter to have generated this data is $\pi$ = `r max_pi `

### Ex 4.4

For the Students data file and corresponding population, find the ML estimate of the population proportion believing in life after death. Construct a Wald 95% confidence interval, using its formula (4.8). Interpret. 

(4.8) $$ \hat \pi = z_{\alpha/2} \sqrt{\frac{\hat \pi (1-\hat \pi)}{n}} $$

```{r}
url <- "https://stat4ds.rwth-aachen.de/data/Students.dat"

students <- read.table(url, header = TRUE)

head(students)
```
```{r}
n = nrow(students)
life_after_death = students$life["life" = 1]

waldInterval = function(x, n, conf.level = 0.95){
   p <- x/n
   sd <- sqrt(p*((1-p)/n))
   z <- qnorm(c( (1 - conf.level)/2, 1 - (1-conf.level)/2)) 
   #returns the value of thresholds at which conf.level has to be cut at. for 95% CI, this is -1.96 and +1.96
   ci <- p + z*sd
   return(ci)
}
waldInterval(life_after_death, n)
```
The interval is extremely small and includes 0: most probably the portion of students that believe in life after death is not significantly different from 0.

### Ex 4.38

For independent observations $y_1, . . . , y_n$ having the geometric distribution (2.1): 

(a) Find a sufficient statistic for $\pi$. 
(b) Derive the ML estimator of $\pi$.

a) 
\[
l(\theta = \pi | x) = \prod_{i=1}^n (1 - \pi )^(y_i - 1)} pi^n
\]
\[
l(\theta = \pi | x) = (1 - \pi )^(y_i - 1)} pi^n
\]
and we can Identify $\pi^n$ as $h(\pi)$, and $ (1 - \pi)^{\sum (y_i - 1)}$ as g, with $\sum (y_i - 1)$ being the sufficient statistic

b)
### Geometric Distribution PMF

The probability mass function (PMF) of a geometric distribution is:

\[
P(X = x) = (1 - p)^{x - 1} p, \quad x = 1, 2, 3, \dots
\]

where:
- \( p \) is the probability of success in a Bernoulli trial.
- \( x \) is the number of trials needed to get the first success.

### Likelihood Function

Given \( n \) independent observations \( x_1, x_2, \dots, x_n \), the likelihood function is the joint probability of the observed data:

\[
L(p) = \prod_{i=1}^n P(X = x_i) = \prod_{i=1}^n \big[(1 - p)^{x_i - 1} p\big]
\]

Simplify:

\[
L(p) = p^n \prod_{i=1}^n (1 - p)^{x_i - 1}
\]

Using the product rule for exponents:

\[
L(p) = p^n (1 - p)^{\sum_{i=1}^n (x_i - 1)}
\]

### Log-Likelihood Function

Take the natural logarithm of the likelihood function to simplify computation:

\[
\ell(p) = \log L(p) = \log\big[p^n (1 - p)^{\sum_{i=1}^n (x_i - 1)}\big]
\]

Using the logarithm properties (\(\log(ab) = \log a + \log b\) and \(\log(a^b) = b \log a\)):

\[
\ell(p) = n \log p + \bigg(\sum_{i=1}^n (x_i - 1)\bigg) \log(1 - p)
\]

### First Derivative

To find the MLE, take the derivative of \(\ell(p)\) with respect to \(p\) and set it equal to 0:

\[
\frac{d\ell(p)}{dp} = \frac{n}{p} - \frac{\sum_{i=1}^n (x_i - 1)}{1 - p} = 0
\]

Rearrange terms:

\[
\frac{n}{p} = \frac{\sum_{i=1}^n (x_i - 1)}{1 - p}
\]

Simplify:

\[
n (1 - p) = p \sum_{i=1}^n (x_i - 1)
\]

\[
n - np = p \sum_{i=1}^n (x_i - 1)
\]

\[
n = np + p \sum_{i=1}^n (x_i - 1)
\]

Factor out \(p\):

\[
n = p \bigg(n + \sum_{i=1}^n (x_i - 1)\bigg)
\]

Solve for \(p\):

\[
p = \frac{n}{n + \sum_{i=1}^n (x_i - 1)}
\]

### Conclusion

The Maximum Likelihood Estimate (MLE) for \(p\) is:

\[
\hat{p} = \frac{n}{\sum_{i=1}^n x_i}
\]

where \( \sum_{i=1}^n x_i \) includes the original observed data values.

### Ex 4.44

Refer to the previous two exercises. Consider the selling prices (in thousands of dollars) in the Houses data file mentioned in Exercise 4.31. 
(a) Fit the normal distribution to the data by finding the ML estimates of $\mu$ and $\sigma$ for that distribution. 

(b) Fit the log-normal distribution to the data by finding the ML estimates of its parameters. 

(c) Find and compare the ML estimates of the mean and standard deviation of selling price for the two distributions. 

(d) Superimpose the fitted normal and log-normal distributions on a histogram of the data. Which distribution seems to be more appropriate for summarizing the selling prices? 

```{r}
url_houses = "https://stat4ds.rwth-aachen.de/data/Houses.dat"

houses <- read.table(url_houses, header = TRUE)

head(houses)
```
#a
It known that the ML estimate of $\mu$ for the Normal distribution is:

```{r}
likelihood_normal <- function(mu, sigma2, data) {
  n <- length(data)
  constant <- 1 / sqrt(2 * pi * sigma2)
  exponent <- exp(-((data - mu)^2) / (2 * sigma2))
  likelihood <- prod(constant * exponent)
  return(-likelihood)
}

mu_hat = mean(houses$price)
s2_hat = var(houses$price)

ML_sigma = optim(
  par = s2_hat,  # Initial guess for sigma^2
  fn = likelihood_normal,
  mu = mu_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

ML_mu = optim(
  par = mu_hat,  # Initial guess for sigma^2
  fn = likelihood_normal,
  sigma = s2_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

comparison <- data.frame(
  Quantity = c("Mean", "Variance"),
  MLE_Function = c(ML_mu$par, ML_sigma$par),
  Built_in_Function = c(mu_hat, s2_hat)
)

comparison
```
```{r}

log_likelihood_normal <- function(mu, sigma2, data) {
  #browser()
  n <- length(data)
  log_constant <- -n / 2 * log(2 * pi * sigma2)  # Logarithm of the constant term
  sum_squared <- sum((data - mu)^2)             # Sum of squared deviations
  log_exponent <- -sum_squared / (2 * sigma2)   # Logarithm of the exponent
  log_likelihood <- log_constant + log_exponent
  return(-log_likelihood)  # Return the negative log-likelihood for optimization
}

log_likelihood_normal(mu_hat, s2_hat, houses$price)

Log_ML_sigma = optim(
  par = s2_hat,  # Initial guess for sigma^2
  fn = log_likelihood_normal,
  mu = mu_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

Log_ML_mu = optim(
  par = mu_hat,  # Initial guess for sigma^2
  fn = log_likelihood_normal,
  sigma = s2_hat,
  data = houses$price,
  method = "L-BFGS-B",
  lower = 1e-6  # Ensure sigma^2 > 0
)

comparison <- data.frame(
  Quantity = c("Mean", "Variance"),
  MLE_Function = c(Log_ML_mu$par, Log_ML_sigma$par),
  Built_in_Function = c(mu_hat, s2_hat)
)

comparison
```
```{r}
# Load required libraries
library(ggplot2)

# Example data: Replace with your data
data <- houses$price

# Fit the Normal distribution
mu_normal <- mean(data)       # Mean
sigma_normal <- sd(data)      # Standard deviation

# Fit the Log-Normal distribution
log_data <- log(data)         # Log-transform the data
mu_log <- mean(log_data)      # Mean of log-transformed data
sigma_log <- sd(log_data)     # Std dev of log-transformed data

# Histogram of the data
ggplot(data = data.frame(price = data), aes(x = price)) +
  geom_histogram(aes(y = ..density..), bins = 30, fill = "lightblue", color = "black") +
  
  # Add Normal PDF
  stat_function(
    fun = dnorm,
    args = list(mean = mu_normal, sd = sigma_normal),
    color = "red",
    size = 1.2,
    aes(linetype = "Normal")
  ) +
  
  # Add Log-Normal PDF
  stat_function(
    fun = function(x) dlnorm(x, meanlog = mu_log, sdlog = sigma_log),
    color = "green",
    size = 1.2,
    aes(linetype = "Log-Normal")
  ) +
  
  # Labels and Legend
  labs(
    title = "Histogram with Fitted Normal and Log-Normal Distributions",
    x = "Selling Prices",
    y = "Density",
    linetype = "Distribution"
  ) +
  theme_minimal()

```

The log-normal distribution seems to be able to capture skewed tails and outliers much better than the standard normal distribution.

### Ex 4.54

Consider $n$ independent observations from an exponential pdf $f(y;\lambda) = \lambda e^{− \lambda y}$ for $y$ $\ge$ 0, with parameter $\lambda$ > 0 for which $E(Y) = \frac{1}{\lambda}$. $\\$

(a) Find the sufficient statistic for estimating $\lambda$. $\\$
(b) Find the maximum likelihood estimator of $\lambda$ and of $E(Y)$. $\\$
(c) One can show that $2 \lambda (\sum_i{Y_i})$ has a chi-squared distribution with $df = 2n$. Explain why
$2 \lambda (\sum_i{Y_i})$ is a pivotal quantity, and use it to derive a 95% confidence interval for $\lambda$.


## FSDS - Chapter 5

### Ex 5.68

Explain why the confidence interval based on the Wald test of $H_0: \theta = \theta_0$ is symmetric around $\hat \theta$ (i.e., having center exactly equal to $\hat \theta$. This is not true for the confidence intervals based on the likelihood-ratio and score tests.) Explain why such symmetry can be problematic when $\theta$ and $\hat \theta$ are near a boundary, using the example of a population proportion that is very close to 0 or 1 and a sample proportion that may well equal 0 or 1.


## FSDS - Chapter 6

### Ex 6.12

For the UN data file at the book’s website (see Exercise 1.24), construct a multiple regression model predicting Internet using all the other variables. Use the concept of multicollinearity to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor. Compare the estimated GDP effect in the bivariate model and the multiple regression model and explain why it is so much weaker in the multiple regression model.

### Ex 6.14

The data set Crabs2 at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, $SpermTotal$, is the log of the total number of sperm in an ejaculate. It has $\bar y$ = 19.3 and $s$ = 2.0. The two explanatory variables used in the R output are the horseshoe crab’s $carapace width$ (CW, mean 18.6 $cm$, standard deviation 3.0 $cm$), which is a measure of its size, and $color$ (1 = dark, 2 = medium, 3 = light), which is a measure of adult age, darker ones being older.

### Ex 6.30

When the values of $y$ are multiplied by a constant $c$, from their formulas, show that $s_y$ and $\hat \beta_1$ in the bivariate linear model are also then multiplied by $c$. Thus, show that r = $\hat \beta_1(\frac{s_x}{s_y})$ does not depend on the units of measurement.

### Ex 6.42

You can fit the quadratic equation E(Y) = $\beta_0 + \beta_1 x + \beta_2 x^2$ by fitting a multiple regression model with $x_1 = x$ and $x_2 = x^2$.

(a) Simulate 100 independent observations from the model $Y = 40.0−5.0x+0.5x^2 + \epsilon$, where X has a uniform distribution over [0, 10] and $\epsilon  ∼ N (0, 1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship

(b) Find the correlation between x and y and explain why it is so weak even though the plot shows a strong relationship with a large $R^2$ value for the quadratic model.

### Ex 6.52

F statistics have alternate expressions in terms of $R^2$ values.

(a) Show that for testing $H_0 : \beta_1 = ... = \beta_p$ = 0, 
$$ F = \frac{(TSS-SSE)/p}{SSE/[n-(p+1)]} $$ 
is equivalent to:

$$ F = \frac{R^2/p}{(1-R^2)/[n-(p+1)]} $$ 
Explain why larger values of $R^2$ yield larger values of $F$.

(b) Show that for comparing nested linear models,

$$ F = \frac{(SSE_0-SSE_1)/(p_1-p_0)}{SSE_1/[n-(p_1+1)]} \quad = \quad \frac{R_1^2-R_0^2/(p_1-p_0)}{(1-R_1^2)/[n-(p_1+1)]} $$


## FSDS - Chapter 7

### Ex 7.4

Analogously to the previous exercise, randomly sample 30 X observations from a uniform in
the interval (-4,4) and conditional on X = $x$, 30 normal observations with E(Y ) = $3.5x^3$ − $20x^2$ + $0.5x$ + 20 and $\sigma$ = 30. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for E(Y ) = $0.5x^3$ − $20x^2$ + $0.5x$ + 20 (same $\sigma$) several times. What do you observe? Which model would you suggest now?

### Ex 7.20

In the Crabs data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).

(a) Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.

(b) Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.

(c) Use AIC to determine which models seem most sensible among the models with (i) interaction, (ii) main effects, (iii) weight as the sole predictor, (iv) color as the sole predictor, and (v) the null model.

### Ex 7.26

A headline in The $Gainesville$ $Sun$ (Feb. 17, 2014) proclaimed a worrisome spike in shark attacks in the previous two years. The reported total number of shark attacks in Florida per year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and negative binomial model fits.

